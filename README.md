# üé® Multi-View_Geometry_and_NeRF_based_3D_Reconstruction

> **Description**: I implemented and tested techniques for 3D scene understanding, including two-view 3D reconstruction from SIFT features and Neural Radiance Fields (NeRF) for novel view synthesis. These projects achieved high-quality results in both classical structure-from-motion reconstruction and implicit neural scene representation.

[![Course](https://img.shields.io/badge/CIS%20580-Machine%20Perception-blue?style=for-the-badge)](https://github.com)
[![Python](https://img.shields.io/badge/Python-3.8+-blue?style=for-the-badge&logo=python)](https://www.python.org/)
[![PyTorch](https://img.shields.io/badge/PyTorch-Deep%20Learning-red?style=for-the-badge&logo=pytorch)](https://pytorch.org/)
[![OpenCV](https://img.shields.io/badge/OpenCV-Computer%20Vision-green?style=for-the-badge&logo=opencv)](https://opencv.org/)

---

## üìã Table of Contents

- [Overview](#-overview)
- [3D Reconstruction using SfM](#-3d-reconstruction-from-two-views)
  - [Technical Approach](#technical-approach)
  - [Key Algorithms](#key-algorithms-hw3)
- [Neural Radiance Fields](#-neural-radiance-fields-nerf)
  - [Technical Approach](#technical-approach)
  - [Key Algorithms](#key-algorithms)
- [Performance Results](#-performance-results) 
- [Lessons Learned](#-lessons-learned)
- [Future Improvements](#-future-improvements)
- [Acknowledgments](#-acknowledgments)

---

## üéØ Overview

This repository contains implementations of two fundamental computer vision projects from CIS 580 (Machine Perception) at the University of Pennsylvania:

### 3D Reconstruction using SfM

**Goal**: Reconstructing 3D scene geometry and camera poses from two 2D images of Satsok Castle using feature matching and epipolar geometry.

**Key Techniques:**
- üîç **SIFT Feature Detection** and matching
- üìê **Essential Matrix Estimation** via 8-point algorithm
- üé≤ **RANSAC** for robust outlier rejection
- üìä **Epipolar Geometry** visualization
- üéØ **Pose Recovery** from essential matrix decomposition
- üó∫Ô∏è **3D Triangulation** from two-view correspondences

### Neural Radiance Fields (NeRF)

**Goal**: Learn implicit 3D scene representations using neural networks to synthesize novel views of scenes.

**Key Techniques:**
- üåä **Positional Encoding** for high-frequency signal reconstruction
- üß† **Multi-Layer Perceptrons** for scene representation
- üì∑ **Volumetric Rendering** equation implementation
- üé® **2D Image Fitting** as warmup task (Van Gogh's Starry Night)
- üß± **3D Scene Rendering** (Lego toy dataset)
- üöÄ **Novel View Synthesis** from limited training views

---

**Course**: CIS 580 - Machine Perception  
**Semester**: Fall 2024  

---

## üì∑ 3D Reconstruction from Two Views

<div align="center">

**From 2D Images to 3D Structure**

*Pipeline: SIFT ‚Üí Matching ‚Üí RANSAC ‚Üí Essential Matrix ‚Üí Pose ‚Üí Triangulation*

</div>

### Technical Approach (HW3)

#### 1. Feature Detection and Matching

**SIFT (Scale-Invariant Feature Transform):**

The pipeline begins by detecting robust keypoints in both images:

```python
# Detect SIFT features
for im in images:
    gray = cv2.cvtColor(im, cv2.COLOR_RGB2GRAY)
    sift = cv2.xfeatures2d.SIFT_create()
    kp, des = sift.detectAndCompute(gray, None)
    keypoints.append(kp)
    descriptions.append(des)
```

**Brute-Force Matching:**

```python
# Match descriptors between images
bf = cv2.BFMatcher(crossCheck=True)
matches = bf.match(descriptions[0], descriptions[1])
```

**Camera Calibration:**

Given intrinsic parameters:
- Focal length: f = 552 pixels
- Principal point: (u‚ÇÄ, v‚ÇÄ) = (307.5, 205)

```python
K = np.array([[f,  0, u0],
              [0,  f, v0],
              [0,  0,  1]])

# Convert to calibrated coordinates
calibrated_1 = np.matmul(K_inv, uncalibrated_1).T
calibrated_2 = np.matmul(K_inv, uncalibrated_2).T
```

#### 2. Essential Matrix Estimation

**Least-Squares 8-Point Algorithm:**

```python
def least_squares_estimation(X1, X2):
    """
    Estimate essential matrix from 8+ point correspondences
    
    For each correspondence (x1, x2), we have:
    x2^T * E * x1 = 0
    """
    N = X1.shape[0]
    A = np.zeros((N, 9))
    
    # Build constraint matrix
    for i in range(N):
        x1, y1, _ = X1[i]
        x2, y2, _ = X2[i]
        A[i] = [x2*x1, x2*y1, x2, y2*x1, y2*y1, y2, x1, y1, 1]
    
    # Solve using SVD
    _, _, Vt = np.linalg.svd(A)
    E = Vt[-1].reshape(3, 3)
    
    # Enforce essential matrix constraint: rank 2
    U, S, Vt = np.linalg.svd(E)
    S = np.diag([1, 1, 0])
    E = U @ S @ Vt
    
    return E
```

**Mathematical Foundation:**

The epipolar constraint states:
```
x2^T * E * x1 = 0
```

Where:
- E is the 3√ó3 essential matrix
- x1, x2 are calibrated image coordinates
- E encodes the relative rotation and translation between cameras

#### 3. RANSAC for Robust Estimation

**Algorithm Overview:**

```python
def ransac_estimator(X1, X2, num_iterations=60000):
    """
    RANSAC algorithm for robust essential matrix estimation
    
    Iteratively:
    1. Sample 8 random correspondences
    2. Estimate E from sample
    3. Count inliers (residual < Œµ)
    4. Keep best E with most inliers
    """
    sample_size = 8
    eps = 1e-4
    best_num_inliers = -1
    best_E = None
    
    # Skew-symmetric matrix for e3 = [0, 0, 1]^T
    e3_cap = np.array([[0, -1, 0],
                       [1,  0, 0],
                       [0,  0, 0]])
    
    for i in range(num_iterations):
        # Random sampling
        permuted = np.random.RandomState(seed=i*10).permutation(X1.shape[0])
        sample_idx = permuted[:sample_size]
        test_idx = permuted[sample_size:]
        
        # Estimate E from sample
        E = least_squares_estimation(X1[sample_idx], X2[sample_idx])
        
        # Count inliers
        inliers = sample_idx.tolist()
        for j in test_idx:
            x1, x2 = X1[j], X2[j]
            
            # Symmetric epipolar distance
            d1_num = (x2.T @ E @ x1)**2
            d1_den = (np.linalg.norm(e3_cap @ E @ x1))**2
            
            d2_num = (x1.T @ E.T @ x2)**2
            d2_den = (np.linalg.norm(e3_cap @ E.T @ x2))**2
            
            residual = (d1_num / d1_den) + (d2_num / d2_den)
            
            if residual < eps:
                inliers.append(j)
        
        # Update best
        if len(inliers) > best_num_inliers:
            best_num_inliers = len(inliers)
            best_E = E
            best_inliers = np.array(inliers)
    
    return best_E, best_inliers
```

**Residual Calculation:**

Distance from point x2 to epipolar line epi(x1):

```
d(x2, epi(x1))¬≤ = (x2^T * E * x1)¬≤ / ||√™3 * E * x1||¬≤
```

Total symmetric distance:
```
residual = d(x2, epi(x1))¬≤ + d(x1, epi(x2))¬≤
```

#### 4. Epipolar Lines Visualization

**Fundamental Matrix:**

For uncalibrated coordinates (pixel space):

```python
def plot_epipolar_lines(image1, image2, uncalibrated_1, uncalibrated_2, E, K):
    """
    Plot epipolar lines in both images
    
    Relationship: F = K^(-T) * E * K^(-1)
    """
    K_inv = np.linalg.inv(K)
    F = K_inv.T @ E @ K_inv
    
    # Epipolar lines in image 2 for points in image 1
    epipolar_lines_in_2 = F @ uncalibrated_1
    
    # Epipolar lines in image 1 for points in image 2
    epipolar_lines_in_1 = F.T @ uncalibrated_2
    
    # [Plotting code...]
```

**Epipolar Constraint (Uncalibrated):**

```
u2^T * F * u1 = 0
```

Where F is the fundamental matrix relating pixel coordinates.

#### 5. Pose Recovery from Essential Matrix

**SVD Decomposition:**

```python
def pose_candidates_from_E(E):
    """
    Extract 4 possible (R, T) configurations from E
    
    Twisted pair ambiguity + sign ambiguity = 4 solutions
    """
    U, _, Vt = np.linalg.svd(E)
    
    # Rotation matrices for ¬±œÄ/2
    Rz_pos = np.array([[0, -1, 0],
                       [1,  0, 0],
                       [0,  0, 1]])
    Rz_neg = Rz_pos.T
    
    # Two rotation solutions
    R1 = U @ Rz_pos.T @ Vt
    R2 = U @ Rz_neg.T @ Vt
    
    # Ensure proper rotations (det = +1)
    if np.linalg.det(R1) < 0: R1 = -R1
    if np.linalg.det(R2) < 0: R2 = -R2
    
    # Two translation solutions
    T1 = U[:, 2]
    T2 = -U[:, 2]
    
    # 4 candidates
    return [
        {"R": R1, "T": T1},
        {"R": R2, "T": T1},
        {"R": R1, "T": T2},
        {"R": R2, "T": T2}
    ]
```

**Mathematical Derivation:**

Essential matrix decomposition:
```
E = [T]_√ó * R = U * Rz(¬±œÄ/2) * Œ£ * U^T
```

Where:
- [T]_√ó is the skew-symmetric matrix of translation T
- R is the rotation matrix
- Rz(Œ∏) is rotation about z-axis by angle Œ∏

#### 6. 3D Triangulation

**Selecting the Correct Pose:**

```python
def reconstruct3D(transform_candidates, calibrated_1, calibrated_2):
    """
    Triangulate 3D points and select pose with most points in front
    
    For each candidate (R, T):
    1. Triangulate all points
    2. Count how many have positive depth in both cameras
    3. Keep candidate with maximum count
    """
    best_num_front = -1
    best_candidate = None
    best_lambdas = None
    
    for candidate in transform_candidates:
        R = candidate['R']
        T = candidate['T']
        
        lambdas = np.zeros((2, calibrated_1.shape[0]))
        
        # Triangulate each correspondence
        for i, (x1, x2) in enumerate(zip(calibrated_1, calibrated_2)):
            # Linear system: Œª2*x2 = Œª1*R*x1 + T
            A = np.zeros((3, 2))
            A[:, 0] = x2
            A[:, 1] = -R @ x1
            b = T
            
            # Least-squares solution
            lambda_vals, _, _, _ = np.linalg.lstsq(A, b, rcond=None)
            lambdas[0, i], lambdas[1, i] = lambda_vals
        
        # Count points with positive depth in both views
        num_front = np.sum(np.logical_and(lambdas[0] > 0, lambdas[1] > 0))
        
        if num_front > best_num_front:
            best_num_front = num_front
            best_candidate = candidate
            best_lambdas = lambdas
    
    # Reconstruct 3D points
    P1 = best_lambdas[1].reshape(-1, 1) * calibrated_1
    P2 = best_lambdas[0].reshape(-1, 1) * calibrated_2
    
    return P1, P2, best_candidate['T'], best_candidate['R']
```

**Triangulation Geometry:**

For a point correspondence (x1, x2):
```
P2 = Œª2 * x2  (in camera 2 frame)
P1 = Œª1 * x1  (in camera 1 frame)

Relationship: P2 = R * P1 + T
```

Substituting:
```
Œª2 * x2 = Œª1 * R * x1 + T
```

This is a linear system in (Œª1, Œª2) that we solve via least-squares.

#### 7. Reprojection Verification

**Forward and Backward Projection:**

```python
def show_reprojections(image1, image2, uncalibrated_1, uncalibrated_2, 
                       P1, P2, K, T, R):
    """
    Verify reconstruction by reprojecting 3D points back to images
    
    Forward: P1 (cam 1) ‚Üí image 2
    Backward: P2 (cam 2) ‚Üí image 1
    """
    P1proj = np.zeros((P1.shape[0], 3))
    P2proj = np.zeros((P2.shape[0], 3))
    
    for i in range(P1.shape[0]):
        # Reproject P1 to camera 2
        P1_transformed = R @ P1[i] + T
        P1proj[i] = K @ P1_transformed
        
        # Reproject P2 to camera 1
        P2_transformed = R.T @ (P2[i] - T)
        P2proj[i] = K @ P2_transformed
    
    # [Plotting code...]
    return P1proj, P2proj
```

### Key Algorithms (HW3)

#### 1. SVD-Based Essential Matrix Estimation

**Input:** N √ó 3 calibrated point correspondences (X1, X2)

**Output:** 3 √ó 3 essential matrix E

**Steps:**
1. Construct N √ó 9 constraint matrix A
2. Compute SVD: A = U Œ£ V^T
3. Extract E from last column of V
4. Project E to essential space: E = U diag(1,1,0) V^T

**Complexity:** O(N) for matrix construction + O(81) for SVD

#### 2. RANSAC Outlier Rejection

**Input:** Point correspondences with outliers

**Output:** Robust essential matrix + inlier set

**Parameters:**
- Sample size: 8 points
- Iterations: 60,000
- Inlier threshold: Œµ = 10^-4

**Probability of Success:**

```
P(success) = 1 - (1 - w^8)^k

Where:
- w = inlier ratio
- k = number of iterations
- w^8 = probability one sample is all inliers
```

For w = 0.5 and k = 60,000:
```
P(success) ‚âà 1.0 (practically guaranteed)
```

#### 3. Cheirality Check

**Purpose:** Disambiguate 4 pose candidates

**Method:** Count points with positive depth in both cameras

```
Valid point if: Œª1 > 0 AND Œª2 > 0
```

**Rationale:** 
- Physical cameras only see points in front (positive depth)
- Incorrect poses will have points behind one or both cameras

---

## üß† Neural Radiance Fields (NeRF)

<div align="center">

**From Neural Networks to 3D Scenes**

*Pipeline: Positional Encoding ‚Üí MLP ‚Üí Volumetric Rendering ‚Üí Novel Views*

</div>

### Technical Approach 

#### Part 1: Fitting a 2D Image

**Goal:** To understand positional encoding and MLP fitting

##### 1.1 Positional Encoding

**Mathematical Definition:**

For input vector x ‚àà ‚Ñù^D, positional encoding Œ≥: ‚Ñù^D ‚Üí ‚Ñù^(2DL)

```
Œ≥(x) = [sin(2^0 œÄ x), cos(2^0 œÄ x), 
        sin(2^1 œÄ x), cos(2^1 œÄ x),
        ...,
        sin(2^(L-1) œÄ x), cos(2^(L-1) œÄ x)]
```

**Implementation:**

```python
def positional_encoding(x, num_frequencies):
    """
    Map input to higher-dimensional space using sinusoids
    
    Args:
        x: [N, D] input coordinates
        num_frequencies: L (number of frequency bands)
    
    Returns:
        encoded: [N, 2*D*L] encoded coordinates
    """
    encoded = []
    
    for freq in range(num_frequencies):
        for func in [torch.sin, torch.cos]:
            encoded.append(func(2**freq * np.pi * x))
    
    return torch.cat(encoded, dim=-1)
```

##### 1.2 MLP Design (2D Image)

**Architecture:**

```python
class MLP(nn.Module):
    def __init__(self, input_dim, output_dim=3, filter_size=256):
        super().__init__()
        
        # Three linear layers
        self.fc1 = nn.Linear(input_dim, filter_size)
        self.fc2 = nn.Linear(filter_size, filter_size)
        self.fc3 = nn.Linear(filter_size, output_dim)
        
        self.relu = nn.ReLU()
        self.sigmoid = nn.Sigmoid()
    
    def forward(self, x):
        x = self.relu(self.fc1(x))
        x = self.relu(self.fc2(x))
        x = self.sigmoid(self.fc3(x))  # Output ‚àà [0, 1]
        return x
```

**Layer Breakdown:**

| Layer | Input Dim | Output Dim | Activation |
|-------|-----------|------------|------------|
| FC1 | 2√ó2√óL | 256 | ReLU |
| FC2 | 256 | 256 | ReLU |
| FC3 | 256 | 3 (RGB) | Sigmoid |

##### 1.3 Training Pipeline

**Objective:** Learn mapping I: ‚Ñù¬≤ ‚Üí ‚Ñù¬≥ (pixel coordinates ‚Üí RGB color)

```python
def train_2d_model(image, num_frequencies, num_iterations=10000):
    """
    Fit MLP to 2D image
    
    Process:
    1. Convert image to (x,y) coordinates
    2. Apply positional encoding
    3. Forward through MLP
    4. Compute MSE loss
    5. Backpropagate and optimize
    """
    H, W, _ = image.shape
    
    # Create coordinate grid
    coords = torch.meshgrid(torch.linspace(0, 1, H), 
                           torch.linspace(0, 1, W))
    coords = torch.stack([coords[1], coords[0]], dim=-1).reshape(-1, 2)
    
    # Positional encoding
    if num_frequencies > 0:
        coords_encoded = positional_encoding(coords, num_frequencies)
    else:
        coords_encoded = coords
    
    # Target colors
    target = image.reshape(-1, 3)
    
    # Training loop
    model = MLP(input_dim=coords_encoded.shape[1])
    optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)
    
    for i in range(num_iterations):
        optimizer.zero_grad()
        
        # Forward pass
        pred_colors = model(coords_encoded)
        
        # Loss
        loss = F.mse_loss(pred_colors, target)
        
        # Backward
        loss.backward()
        optimizer.step()
        
        # Compute PSNR
        if i % 1000 == 0:
            psnr = 10 * torch.log10(1.0 / loss)
            print(f"Iter {i}, PSNR: {psnr:.2f} dB")
    
    # Reconstruct image
    with torch.no_grad():
        reconstructed = model(coords_encoded).reshape(H, W, 3)
    
    return reconstructed
```

**PSNR Metric:**

Peak Signal-to-Noise Ratio:

```
PSNR = 10 ¬∑ log‚ÇÅ‚ÇÄ(R¬≤ / MSE)

Where:
- R = maximum pixel value (1.0 for normalized images)
- MSE = mean squared error
```

Higher PSNR = better reconstruction quality

**Results:**

| Frequencies (L) | PSNR @ 10k iters | Quality |
|----------------|------------------|---------|
| 0 (no encoding) | 15.5 dB | Blurry |
| 2 | 16.2 dB | Improved |
| 6 | 26+ dB | Sharp details |

#### Part 2: Fitting a 3D Scene (NeRF)

##### 2.1 Computing Image Rays

**Camera Model:**

For each pixel (u, v), compute ray in world coordinates:

```python
def get_rays(height, width, intrinsics, pose):
    """
    Generate rays for all pixels in an image
    
    Args:
        height, width: Image dimensions
        intrinsics: K matrix (3√ó3)
        pose: Camera-to-world transform (4√ó4)
    
    Returns:
        ray_origins: [H, W, 3] - same for all pixels
        ray_directions: [H, W, 3] - varies per pixel
    """
    # Camera origin in world frame
    ray_origins = pose[:3, 3].expand(height, width, 3)
    
    # Pixel coordinates
    i, j = torch.meshgrid(torch.arange(height), torch.arange(width))
    
    # Normalized image coordinates
    dirs = torch.stack([
        (j - intrinsics[0, 2]) / intrinsics[0, 0],
        (i - intrinsics[1, 2]) / intrinsics[1, 1],
        torch.ones_like(i)
    ], dim=-1).float()
    
    # Transform to world frame
    ray_directions = (pose[:3, :3] @ dirs.reshape(-1, 3).T).T
    ray_directions = ray_directions.reshape(height, width, 3)
    
    return ray_origins, ray_directions
```

**Ray Equation:**

```
r(t) = o + t¬∑d

Where:
- o = ray origin (camera center)
- d = ray direction (through pixel)
- t = depth parameter
```

##### 2.2 Stratified Sampling

**Sampling Points Along Ray:**

```python
def stratified_sampling(ray_origins, ray_directions, near, far, samples):
    """
    Sample points along rays with stratification
    
    Divide [near, far] into N bins and sample uniformly within each
    
    Args:
        ray_origins: [H, W, 3]
        ray_directions: [H, W, 3]
        near, far: Depth bounds
        samples: Number of samples N
    
    Returns:
        sample_points: [H, W, N, 3]
        depth_values: [H, W, N]
    """
    # Stratified depth values
    t_vals = torch.linspace(near, far, samples)
    depth_values = t_vals.unsqueeze(0).unsqueeze(0).expand(
        ray_origins.shape[0], ray_origins.shape[1], samples
    )
    
    # Points along rays: r(t) = o + t*d
    sample_points = ray_origins.unsqueeze(2) + \
                   depth_values.unsqueeze(-1) * ray_directions.unsqueeze(2)
    
    return sample_points, depth_values
```

**Stratification Formula:**

```
t_i = t_near + (i-1)/N ¬∑ (t_far - t_near),  i = 1, ..., N
```

Ensures uniform coverage along the ray.

##### 2.3 NeRF MLP Architecture

**Network Design:**

```python
class NeRF(nn.Module):
    def __init__(self, pos_enc_dim=60, dir_enc_dim=24):
        super().__init__()
        
        # Position encoding network (8 layers)
        self.fc1 = nn.Linear(pos_enc_dim, 256)
        self.fc2 = nn.Linear(256, 256)
        self.fc3 = nn.Linear(256, 256)
        self.fc4 = nn.Linear(256, 256)
        # Skip connection
        self.fc5 = nn.Linear(256 + pos_enc_dim, 256)
        self.fc6 = nn.Linear(256, 256)
        self.fc7 = nn.Linear(256, 256)
        self.fc8 = nn.Linear(256, 256)
        
        # Density head (no activation)
        self.density = nn.Linear(256, 1)
        
        # Feature extraction
        self.fc9 = nn.Linear(256, 256)
        
        # Direction-dependent color (with direction encoding)
        self.fc10 = nn.Linear(256 + dir_enc_dim, 128)
        self.fc11 = nn.Linear(128, 3)
        
        self.relu = nn.ReLU()
        self.sigmoid = nn.Sigmoid()
    
    def forward(self, x_encoded, d_encoded):
        """
        Args:
            x_encoded: [N, 60] position encoding
            d_encoded: [N, 24] direction encoding
        
        Returns:
            rgb: [N, 3] color
            sigma: [N, 1] density
        """
        # Position-dependent layers
        h = self.relu(self.fc1(x_encoded))
        h = self.relu(self.fc2(h))
        h = self.relu(self.fc3(h))
        h = self.relu(self.fc4(h))
        
        # Skip connection
        h = torch.cat([h, x_encoded], dim=-1)
        h = self.relu(self.fc5(h))
        h = self.relu(self.fc6(h))
        h = self.relu(self.fc7(h))
        h = self.relu(self.fc8(h))
        
        # Density (view-independent)
        sigma = self.density(h)
        
        # Feature for color
        h = self.fc9(h)
        
        # Direction-dependent color
        h = torch.cat([h, d_encoded], dim=-1)
        h = self.relu(self.fc10(h))
        rgb = self.sigmoid(self.fc11(h))
        
        return rgb, sigma
```


**Design Principles:**

1. **Density before direction**: œÉ computed without viewing direction
   - Enforces view-independent geometry
   
2. **Skip connection**: Concatenate input at layer 5
   - Helps gradient flow
   - Preserves high-frequency details

3. **Separate color head**: Viewing direction only affects color
   - Models non-Lambertian reflectance

##### 2.4 Volumetric Rendering

**Rendering Equation:**

Continuous form:
```
C(r) = ‚à´[t_near to t_far] T(t) ¬∑ œÉ(r(t)) ¬∑ c(r(t), d) dt

Where:
T(t) = exp(-‚à´[t_near to t] œÉ(r(s)) ds)
```

**Discrete Approximation:**

```python
def volumetric_rendering(rgb, sigma, depth_values):
    """
    Render ray color from sampled points
    
    Args:
        rgb: [H, W, N, 3] - colors at sample points
        sigma: [H, W, N, 1] - densities at sample points
        depth_values: [H, W, N] - depth samples
    
    Returns:
        rendered_color: [H, W, 3]
    """
    # Distance between samples
    delta = depth_values[:, :, 1:] - depth_values[:, :, :-1]
    delta = torch.cat([delta, torch.ones_like(delta[:, :, :1]) * 1e9], dim=-1)
    
    # Alpha values: 1 - exp(-œÉ¬∑Œ¥)
    sigma = F.relu(sigma)  # Ensure non-negative
    alpha = 1.0 - torch.exp(-sigma * delta.unsqueeze(-1))
    
    # Transmittance: T_i = exp(-Œ£[j<i] œÉ_j¬∑Œ¥_j)
    transmittance = torch.cumprod(
        torch.cat([torch.ones_like(alpha[:, :, :1]), 
                   1.0 - alpha[:, :, :-1]], dim=2),
        dim=2
    )
    
    # Weights: w_i = T_i ¬∑ Œ±_i
    weights = transmittance * alpha
    
    # Composite color: C = Œ£ w_i ¬∑ c_i
    rendered_color = torch.sum(weights * rgb, dim=2)
    
    return rendered_color
```

**Numerical Formula:**

```
ƒà(r) = Œ£[i=1 to N] T_i ¬∑ (1 - exp(-œÉ_i¬∑Œ¥_i)) ¬∑ c_i

Where:
T_i = exp(-Œ£[j=1 to i-1] œÉ_j¬∑Œ¥_j)
Œ¥_i = t_{i+1} - t_i
Œ¥_N = 10^9 (infinity)
```

**Key Implementation Details:**

1. **ReLU on sigma**: Ensures œÉ ‚â• 0 (physical constraint)

2. **cumprod for transmittance**: Efficient O(N) computation
   - Avoids O(N¬≤) nested loop

3. **Large delta_N**: Approximates infinite ray extent

##### 2.5 Complete Forward Pass

```python
def one_forward_pass(H, W, intrinsics, pose, near, far, samples, 
                     model, num_x_freq, num_d_freq):
    """
    Render one image through NeRF pipeline
    
    Steps:
    1. Compute rays for all pixels
    2. Sample points along rays
    3. Encode positions and directions
    4. Forward through network (in chunks)
    5. Volumetric rendering
    
    Returns:
        rendered_image: [H, W, 3]
    """
    # Step 1: Rays
    ray_origins, ray_directions = get_rays(H, W, intrinsics, pose)
    
    # Step 2: Sampling
    sample_points, depth_values = stratified_sampling(
        ray_origins, ray_directions, near, far, samples
    )
    
    # Step 3: Encode
    # Flatten for network
    points_flat = sample_points.reshape(-1, 3)
    dirs_flat = ray_directions.unsqueeze(2).expand(-1, -1, samples, -1).reshape(-1, 3)
    
    # Normalize directions
    dirs_flat = dirs_flat / torch.norm(dirs_flat, dim=-1, keepdim=True)
    
    # Positional encoding
    points_encoded = positional_encoding(points_flat, num_x_freq)
    dirs_encoded = positional_encoding(dirs_flat, num_d_freq)
    
    # Step 4: Network forward (chunked to avoid OOM)
    chunk_size = 1024 * 64
    rgb_chunks = []
    sigma_chunks = []
    
    for i in range(0, points_encoded.shape[0], chunk_size):
        rgb_chunk, sigma_chunk = model(
            points_encoded[i:i+chunk_size],
            dirs_encoded[i:i+chunk_size]
        )
        rgb_chunks.append(rgb_chunk)
        sigma_chunks.append(sigma_chunk)
    
    rgb = torch.cat(rgb_chunks, dim=0).reshape(H, W, samples, 3)
    sigma = torch.cat(sigma_chunks, dim=0).reshape(H, W, samples, 1)
    
    # Step 5: Render
    rendered_image = volumetric_rendering(rgb, sigma, depth_values)
    
    return rendered_image
```

##### 2.6 Training Loop

```python
def train_nerf(images, poses, intrinsics, iterations=3000):
    """
    Train NeRF on multiple views
    
    Strategy:
    - Random image selection each iteration
    - MSE loss between rendered and target
    - Adam optimizer with lr=5e-4
    """
    model = NeRF().to(device)
    optimizer = torch.optim.Adam(model.parameters(), lr=5e-4)
    
    for i in range(iterations):
        # Random training image
        idx = torch.randint(0, len(images), (1,)).item()
        target_image = images[idx]
        target_pose = poses[idx]
        
        # Render
        rendered = one_forward_pass(
            H, W, intrinsics, target_pose, 
            near, far, samples, model,
            num_x_freq=10, num_d_freq=4
        )
        
        # Loss
        loss = F.mse_loss(rendered, target_image)
        
        # Optimize
        optimizer.zero_grad()
        loss.backward()
        optimizer.step()
        
        # Validate on held-out view
        if i % 100 == 0:
            with torch.no_grad():
                test_rendered = one_forward_pass(
                    H, W, intrinsics, test_pose, ...
                )
                psnr = -10 * torch.log10(F.mse_loss(test_rendered, test_image))
                print(f"Iter {i}, PSNR: {psnr:.2f} dB")
    
    return model
```

**Training Hyperparameters:**

| Parameter | Value |
|-----------|-------|
| Learning rate | 5e-4 |
| Optimizer | Adam |
| Samples per ray | 64 |
| Near plane | 2.0 |
| Far plane | 6.0 |
| Batch size | 1 image |
| Iterations | 3000 |


### Key Algorithms 

#### 1. Positional Encoding

**Purpose:** Enable learning of high-frequency functions

**Mathematical Form:**
```
Œ≥(p) = [sin(2^0¬∑œÄ¬∑p), cos(2^0¬∑œÄ¬∑p), ..., sin(2^(L-1)¬∑œÄ¬∑p), cos(2^(L-1)¬∑œÄ¬∑p)]
```

**Properties:**
- Dimensionality: D ‚Üí 2DL
- Fourier feature mapping
- Captures multi-scale patterns

#### 2. Stratified Ray Sampling

**Purpose:** Uniform coverage of ray depth

**Algorithm:**
1. Divide [t_near, t_far] into N bins
2. Sample uniformly within each bin
3. Ensures adequate sampling density

**Benefits:**
- Prevents clustering
- Better gradient signal
- Stable training

#### 3. Volume Rendering Integration

**Discrete Quadrature:**

```
C(r) ‚âà Œ£[i=1 to N] w_i ¬∑ c_i

Where:
w_i = T_i ¬∑ Œ±_i
T_i = Œ†[j=1 to i-1] (1 - Œ±_j)
Œ±_i = 1 - exp(-œÉ_i ¬∑ Œ¥_i)
```

**Efficient Computation:**
- Use torch.cumprod() for O(N) transmittance
- Vectorized operations
- GPU-friendly

---

## üìä Performance Results

### 3D Reconstruction

**SIFT Matching:**
- Initial matches: ~1000+ correspondences
- RANSAC inliers: ~600-800 (60-80% inlier ratio)
- Computation time: ~30-60 seconds (60k RANSAC iterations)

**Essential Matrix Quality:**
- Epipolar distance (inliers): < 1 pixel
- Geometric error: < 0.01 (normalized coordinates)

**3D Reconstruction:**
- Points in front of both cameras: ~95%+
- Reprojection error: ~2-5 pixels
- Reconstruction up to scale (unknown baseline)

### NeRF

#### 2D Image Fitting (Starry Night)

| Config | Frequencies (L) | PSNR @ 10k | Visual Quality |
|--------|----------------|------------|----------------|
| Baseline | 0 | 15.5 dB | Blurry, low-freq only |
| Low-freq | 2 | 16.2 dB | Some details |
| High-freq | 6 | **26+ dB** | Sharp, faithful |

**Training Time:**
- 10,000 iterations: ~5-10 minutes (GPU)
- Convergence: ~5000 iterations

#### 3D Scene (Lego Toy)

**Final Performance:**
- PSNR @ 3000 iterations: **25 dB**
- Target PSNR (paper): 40 dB (with advanced techniques)
- Training time: ~2-2.5 hours (GPU)
- Novel view quality: Good geometry, some blur

**Comparison:**

| Method | PSNR | Training Time | Notes |
|--------|------|---------------|-------|
| Our Implementation | 25 dB | 2-2.5 hours | Basic NeRF |
| Original Paper | 40 dB | 24+ hours | + hierarchical sampling, fine network |
| Instant-NGP | 35+ dB | ~5 minutes | Hash encoding |


---

## üìö Lessons Learned

#### ‚úÖ What Worked Well

1. **RANSAC Robustness**
   - 60,000 iterations provided excellent outlier rejection
   - Achieved 60-80% inlier ratio consistently
   - Geometric distance metric worked better than algebraic

2. **SVD for Essential Matrix**
   - Stable numerical solution
   - Enforcing rank-2 constraint critical for physical validity

3. **Cheirality Check**
   - Simple but effective for pose disambiguation
   - 95%+ of points correctly in front after selection

4. **Modular Pipeline**
   - Each component (LSE, RANSAC, triangulation) independently testable
   - Easy to debug and iterate

5. **Positional Encoding Impact**
   - Dramatic improvement: 15dB ‚Üí 26dB PSNR
   - Critical for representing fine details
   - L=6 sufficient for 2D images

6. **Skip Connection**
   - Enabled deeper network (10 layers)
   - Preserved high-frequency information
   - Faster convergence

7. **Stratified Sampling**
   - Better than uniform random sampling
   - More efficient use of network capacity

8. **Adam Optimizer**
   - Faster than SGD for this problem
   - Learning rate 5e-4 worked well

#### ‚ö†Ô∏è Challenges Encountered

1. **Feature Matching Quality**
   - SIFT occasionally produced symmetric mismatches
   - Ratio test (Lowe's criterion) would have helped
   - **Improvement**: Add ratio test: `d1/d2 < 0.8`

2. **Numerical Stability**
   - Some point correspondences led to nearly singular systems
   - **Solution**: Added regularization in least-squares

3. **Outlier Sensitivity**
   - Even with RANSAC, a few outliers remained
   - **Improvement**: Iterative refinement after RANSAC

4. **Training Time**
   - 3000 iterations √ó 0.6s = ~30 minutes
   - Full quality (40dB) would require 100k+ iterations
   - **Lesson**: Trade-off between time and quality

5. **Memory Constraints**
   - Full image √ó 64 samples √ó network = OOM
   - **Solution**: Chunking (64k points at a time)
   - Still limits batch size to 1 image

---

## üîÆ Future Improvements

### 3D Reconstruction

#### Short-Term

1. **Better Feature Matching**
   ```python
   # Lowe's ratio test
   good_matches = []
   for m, n in matcher.knnMatch(des1, des2, k=2):
       if m.distance < 0.8 * n.distance:
           good_matches.append(m)
   ```

2. **Bundle Adjustment**
   - Jointly optimize all camera poses and 3D points
   - Minimize reprojection error
   - Libraries: OpenCV, Ceres Solver

3. **Multi-View Reconstruction**
   - Extend to 3+ views
   - Incremental SfM pipeline
   - Dense reconstruction (MVS)

#### Long-Term

1. **Deep Learning Features**
   - Replace SIFT with SuperPoint/SuperGlue
   - More robust to viewpoint/lighting changes

2. **Learned Matching**
   ```python
   # SuperGlue-style
   matches, confidence = superglue(features1, features2)
   inliers = matches[confidence > threshold]
   ```

### NeRF

#### Short-Term

1. **Hierarchical Sampling**
   ```python
   # Coarse network
   weights_coarse = render_coarse(samples_coarse)
   
   # Fine samples based on coarse weights
   samples_fine = importance_sampling(weights_coarse)
   
   # Fine network
   rgb_fine = render_fine(samples_fine)
   ```

2. **Learned Positional Encoding**
   ```python
   # Hash-based encoding (Instant-NGP)
   class HashEncoding(nn.Module):
       def __init__(self, num_levels=16, features_per_level=2):
           self.hash_tables = [create_hash_table() for _ in range(num_levels)]
       
       def forward(self, x):
           encoded = []
           for level, table in enumerate(self.hash_tables):
               encoded.append(table.lookup(x * 2**level))
           return torch.cat(encoded, dim=-1)
   ```

3. **Appearance Embedding**
   - Per-image latent code
   - Model lighting variations
   - Better multi-view consistency

#### Long-Term

1. **Real-Time Rendering**
   - Bake NeRF into efficient representation
   - Octree-based sparse voxels
   - Neural graphics primitives

2. **Dynamic Scenes**
   - Add time as input dimension
   - Model non-rigid deformations
   - Video novel view synthesis

3. **Generative Models**
   - NeRF as prior in 3D GANs
   - Text-to-3D synthesis
   - Few-shot novel objects

---

## üôè Acknowledgments

- **CIS 580 Teaching Staff** for excellent course materials and support
- **University of Pennsylvania** for providing computational resources
- **Original NeRF Authors** (Mildenhall et al.) for groundbreaking work
- **OpenCV Community** for robust computer vision tools
- **PyTorch Team** for deep learning framework

**Special Thanks:**
- Course instructor for clear explanations of epipolar geometry
- TAs for debugging help during office hours
- Fellow students for discussion and collaboration

---
<div align="center">

[‚¨Ü Back to Top](#-Multi-View_Geometry_and_NeRF_based_3D_Reconstruction)

</div>

---
